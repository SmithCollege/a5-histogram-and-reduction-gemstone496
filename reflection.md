1. What went well?
 - Histogram was the fastest assignment of all time. Not a single error. No issues. I pulled the <random> module from somewhere online and it worked perfectly following the documentation.
 - Also, the graphing and data collection went super fast this time around (thank past me for setting up files that were super easy to adjust to collect large amounts of data and run everything super efficiently)
2. What was difficult?
 - The reduction piece of this assignment took FOR.EVER. Every bug in the damn world. Also, somewhere around reducing like 1,000,000 it decided that it no longer was able to remember anything and started adding every block to be 2048 regardless of whether the block should have stopped at size. This was a super weird bug that I actually found to be unfixable I think? It felt like the kind of bug where it was just a matter of dealing-with-large-numbers rather than bad code and I didn't have enough time to figure out how to deal with big numbers correctly so I left it as is (and the bug wasn't related to flipping in/outputs, because it occured starting from iteration 1, just only when SIZE > 1,000,000).
3. How would you approach differently?
 - Y'know, I really wanna try to figure out how to do more fancy C++ type stuff w CUDA. It compiles as a C++ file, so I should be able to, for example, submit a map into CUDA. Right? I wanna try figuring out how to do a histogram using a proper hashmap instead of having to allocate a whole bin for every possible char even though they might not all be used. That being said, idt that I had many issues with this assignment and don't have much I'd change.
4. Anything else you [jade lilian] want me [prof robson] to know?
 - As mentioned above, the reduction code is slightly bugged, but only at super high inputs. I am unfortunately unable to identify this bug (trust me, I spent like 2 days on it). The bug is this: at high input values, the kernel seems to stop realizing once it has passed its designated size to stop counting. As a result, if your input size is > 1,000,000; the kernel will give you a result that is close to appropriate, but (sometimes) rounds up your sum to the nearest multiple of 2048. The bug occurs inconsistently on the first run of each code, but will always occur on future runs. I am confident I was supplying my correct sizes and clobbering my data appropriately between rounds, so I do not know why run # affects the likelihood of the bug occuring. Since it is inconsistent and I can't identify an issue with my code, I have decided to submit it, with data collected using the (only slightly) bugged code. 